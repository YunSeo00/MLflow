{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://192.168.0.14:9000\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://192.168.0.14:5001\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"miniostorage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkMlflowExampleApp\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 불러오기\n",
    "filePath = \"./data/sf-airbnb-clean.parquet\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed = 42)\n",
    "\n",
    "# 2. 범주형 변수 라벨링\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols = categoricalCols,\n",
    "                              outputCols = indexOutputCols,\n",
    "                              handleInvalid = \"skip\")\n",
    "\n",
    "# 3. 범주형 변수 + 수치형 변수 --(vectorassembelr)--> \"features\" 변수 생성\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols = assemblerInputs,\n",
    "                               outputCol = \"features\")\n",
    "# 4. 모델 생성\n",
    "rf = RandomForestRegressor(labelCol = \"price\", maxBins = 40, maxDepth = 5, numTrees = 10, seed = 42)\n",
    "\n",
    "# 5. Pipeline 생성\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = \"random-forest\") as run:\n",
    "    # 1. 매개변수 저장\n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "\n",
    "    # 2. 모델 저장\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # 3. Metric(성능지표) 저장 - rmse, r2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # 4. 아티팩트(artifact) 정보 - feature importance\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                      rfModel.featureImportances)),\n",
    "                             columns=[\"feature\", \"importance\"])\n",
    "                .sort_values(by = \"importance\", ascending = False))\n",
    "\n",
    "    # * 먼저 로컬에 파일을 저장하고 MLflow에 파일 경로를 입력하여 저장해야 함. * \n",
    "    pandasDF.to_csv(\"feature-importance.csv\", index = False)\n",
    "    mlflow.log_artifact(\"feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
