{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkMlflowExampleApp\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://192.168.0.14:9000\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://192.168.0.14:5001\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"miniostorage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment=dict(mlflow.get_experiment_by_name('BOAZ-Test'))\n",
    "experiment_id=current_experiment['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/1', creation_time=1680956716995, experiment_id='1', last_update_time=1680956716995, lifecycle_stage='active', name='BOAZ-Test', tags={}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실험 환경 가져오기\n",
    "experiment = client.get_experiment(experiment_id)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47eeb5ab5800448e9646e62d493dbb3e\n",
      "upset-lynx-684\n",
      "{'rmse': 234.9094951959742, 'r2': 0.04766391241134604}\n",
      "{'num_trees': '10', 'max_depth': '5'}\n"
     ]
    }
   ],
   "source": [
    "# 실행 검색\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(experiment_ids = experiment.experiment_id,\n",
    "                         order_by= [\"metrics.rmse\"], # rmse가 작은 모델부터 검색\n",
    "                          max_results = 3)\n",
    "run_id = runs[0].info.run_id\n",
    "run_name = runs[0].info.run_name\n",
    "print(run_id)\n",
    "print(run_name)\n",
    "print(runs[0].data.metrics)\n",
    "print(runs[0].data.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/08 13:00:29 INFO mlflow.spark: 'runs:/47eeb5ab5800448e9646e62d493dbb3e/rf1' resolved as 's3://mlflow/1/47eeb5ab5800448e9646e62d493dbb3e/artifacts/rf1'\n",
      "2023/04/08 13:00:30 INFO mlflow.spark: URI 's3://mlflow/1/47eeb5ab5800448e9646e62d493dbb3e/artifacts/rf1/sparkml' does not point to the current DFS.\n",
      "2023/04/08 13:00:30 INFO mlflow.spark: File 's3://mlflow/1/47eeb5ab5800448e9646e62d493dbb3e/artifacts/rf1/sparkml' not found on DFS. Will attempt to upload the file.\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "pipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "inputDF = spark.read.parquet(\"./data/sf-airbnb-clean.parquet\")\n",
    "predDF = pipelineModel.transform(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.077723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241.721852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.119017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction\n",
       "0  165.077723\n",
       "1  241.721852\n",
       "2  109.119017"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predDF.toPandas()[['prediction']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
